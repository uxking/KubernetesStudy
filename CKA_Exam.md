# Certified Kubernetes Administrator (CKA)

## Intro
Test is hands on - no multiple choice  

Update coming 2019 1st QTR for version 1.12.0  

Download the handbook from github  

Objectives: 
- Scheduling
- logging and monitoring
- application lifecycle management
- cluster, security, storage, troubleshooting, core concepts, networking, installation, configuration and validation.  

#### Cloud Playground (command to join nodes to cluster)
`kubeadm join 172.31.108.193:6443 --token 866cng.ure2k193xwxvmgl0 --discovery-token-ca-cert-hash sha256:b15c34c7c169cdfe6ca8f0bc385401dd1db7f57fa95e5b04eac39939948bec58`  

## Bare Image Install on CentOS

    sudo swapoff -a 
    sudo vi /etc/fstab # comment out #/root/swap

Install and config Docker

    sudo yum -y install docker
    sudo systemctl enable docker
    sudo systemctl start docker

Add Kubernetes repo  

    cat << EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
    enabled=1
    gpgcheck=1
    repo_gpgcheck=1
    gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
    EOF

Turn of selinux

    sudo setenforce 0
    sudo vi /etc/selinux

Change the line that says `SELINUX=enforcing` to `SELINUX=permissive` and save the file.

Install Kubernetes Components

    sudo yum install -y kubelet kubeadm kubectl
    sudo systemctl enable kubelet
    sudo systemctl start kubelet

Configure sysctl  

    cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    EOF
    
    sudo sysctl --system

Initialize the Kube Master. Do this only on the master node

    sudo kubeadm init --pod-network-cidr=10.244.0.0/16
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

Install flannel networking

    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

The `kubeadm init` command that you ran on the master should output a `kubeadm join` command containing a token and hash. You will need to copy that command from the master and run it on all worker nodes with `sudo`.

    sudo kubeadm join $controller_private_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

Now you are ready to verify that the cluster is up and running. On the Kube Master server, check the list of nodes.

    kubectl get nodes

It should look something like this:

    NAME                      STATUS   ROLES    AGE     VERSION
    wboyd4c.mylabserver.com   Ready    master   3m36s   v1.12.2
    wboyd5c.mylabserver.com   Ready    <none>   23s     v1.12.2

Make sure that all of your nodes are listed and that all have a STATUS of `Ready`.

### Kubernetes API Primitives and Cluster Architecture

Records of intent - ensures object will exist, number of instances, restart descriptions, etc..

Contains Object Spec and Object Status

Spec: is our desired state

Status: is the actual state of the object

**Kubernetes YAML**

JSON converted from YAML file

Common Kubernetes Objects

- Nodes
- Pods
- Deployments
- Services
- ConfigMaps

**Names and UIDs**

Names:
- all have unique names
- max length of 253 chars
- dash - and period . allowed

UIDs:
- all objectshave unique uid
- generated by k8s
- spatially and temporarily unique

Namespaces:
- multiple virtual clustrers
- large deployments usually
- provide scopes
- same policy
- kube-system namespace for system pods
- allow for resource quotas

Nodes:
- any worker machine
- may  be VM or physical
- managed by master
- container runtime, kublet, kubeproxy
- not inherently created by k8s, but by cloud providers
- k8s checks node for validity

Cloud Controller Managers:
- route controller (gce clusters only)
- service controller
    - listens for service creation calls, lb, etc..
- PersistantVolumeLabels controller
    - applies labels on AWS EBS and GCE Disks (needs labels)

Node Controller:
- CIDR block assignment
- Keeps tracks of nodes
- Monitors the node health
- Evicts pods if unhealthy 
- Can taint nodes based on conditions

Note: you should spread nodes across AZs

### Kubernetes Services and Network Primitives
**Kubernetes Services**
Pods:
- are the simplest k8s object, one or more containers running on a single node
- Ephemeral, disposable and replacable --stateless
- Not usually managed directly

Deployments:
- Deployment specs
	- image to run
	- number of replicas
Services:
- Deployment of exposed services
- what port is exposed - load balancing and port forwarding types

If pod network IP method: deployment has a single IP
- kubeproxy handles the load balancing - directs traffic

___Kubernetes is both imperative and declarative___

**YAML**
Contain:
apiVersion:
kind:
metadata:
	name: (define the namespace, or blank = default)

Ingress:
- load balancers and ingress controllers

Jobs are applications to run inside pods. Good for batch processing
Running Jobs:
`kubectl create -f <filename.yaml>`

Running Pods:
`kubectl apply -f <filename.yaml>`

### Other Deployments
- Minikube Single node k8s cluster on local workstation
- Kubeadm - multi-node localy, but challenging
	- Would need own CNI (Cluster Network Interface)
- Ubuntu on LXD 9 cluster host on local machine
- Of course, google, azure, AWS, IBM, etc...
- Turnkey solutions are available on cloud providers
### Add-on
- CNI - calico, l3 networking
- Canal - unites Flanel and Calico
- Contiv - native L3 BGP (open source)
- Flannel - Overlay network provider
- Multus - multi plugin for multiple network support
- NSX-T integrates VMWare NXT

#### Cluster Management
`kubectl get pods`  
`kubectl describe pods`  
`kubectl get nodes`  
`kubectl describe nodes`  
`kubectl describe <node|pod|etc> <name>`  
`kubectl get pods --namespace=kube-system`  
### Hardware
- Nodes can be physical or virtual
- Need common network, 443 communication to the internet can work
- Flannel is needed to allow pods to communicat (or other networking application)
- Will use overlay network
- Understand the relationship between master and nodes
- kubectl can be installed on local machine - not only the master
### Securing Communications
- To API, control-plane and pod-to-pod
- All goes through API driver
- Defauilt encryption is TLS
- Most installs handle certificate creation
- Some methods might enable local ports over HTTP - doublecheck
- All that connect to API should be authenticated
- Once, authentciated, every API call should pass an auth check
- Role-Based Access
	- K8s has RBAC that maps users to roles
	- Certain roles can perform specific actions
	- Several roles already created
	- Namespaces can be used to limit role access
	- Can create resources via a deployment (but may not have access to create a pod), can be tricky here.
- Securing the Kubelet
	- Do on each node
	- Expose HTTPS endpoints wich give access to both data and actions - by default they are open
	- use `--anonymous-auth=false` flag and use x509 certs in the configuration
- Restrict access to the network via a namespace
- Network policies would be needed
- Pod CNI must respect policies - most do, read documentation of add-on
- Can assign quotas or limit ranges to users
	- Some plugins may be found
- Other vulnerabilities
	- Etcd - stores configuration and secrets - strong creds needed here
		- maybe secure behind a firewall
	- Audit logging - in beta
		- Records actions taken by the API
		- move audit logs to secure location
- Rotate infrastructure credentails regularly
	- tools can be used to automate this
- Always review 3rd party integrations
	- don't allow them access to the  kube-system namespace
### HA
- Create reliable nodes
- Setup redundant storage with multinode deployment of etcd
- Start replicated and load balanced API servers
- Setup master-elected k8s scheduler and controller manager daemons
- all communications to the master API server needs to go via the load balancer
- First Steps
	- Master node reliability
	- auto restart
	- kubelet already does this, so if kubelet goes down, need to restart it
	- monit, systems, systemctl tools are good tools for this on the linux side
- Second Step
	- need persistant, redunant storage
	- Clusted etcd already replicates to master instances in the cluster
	- all master nodes would need lose disk
	- increase size of cluster to help mitigate
	- possibly use cloud providers persitant disk
	- physical machines, nfs, or iSCSI, or gluster or ceph
- Third Step
	- need a log file for docker to mount
	- `touch /var/log/kube-aiserver.log`
	- create a /srv/kubernetes/ dir on each node
	- Add files
		- basic_auth.csv
		- ca.crt
		- known_tokens.csv
		- kubecft.crt
		- kubecfg.key
		- server.cert
		- server.key
	- Create manually or copy from master noded on a working cluster
	- copy `kube-apiserver.yaml` in to `/etc/kubernetes/manifists` on each of the master nodes
	- kubelet monitors and will create an instance of kube-apiserver container using definition in `yaml` file.
	- Should have 3 API servers at this point
	- Setup load blancer will depend on specifis
	- For external users of API (kubectl, pipelins, other clients) need to talk to external load balancers IP Address
- Fourth Step
	- Allow state changes (use lease-lock)
	- Controller managers and scheduler launched with flag `--leader-elect` flag
	- best to configure commuinications to load balanced IP address of API server (accessing the API via 127.0.0.0 may be problematic if node is unavaiable
- Installing these configs
	- create empty files on each node that docker will mount
		- touch `/var/log/kube-scheduler.log` and `/var/log/kube-controller-manager.log`
		- copy `kube-scheduler.yaml` and `kube-controller-manager.yaml` to `/etc/kubernetes/manifests/`
	- Should be all that is needed to go HA 
### Validating Nodes and Cluster
- End-to-end testing
	- Primarily a dev tool
	- GCE has its own
	- juju-deployed tests
- Kubetest Suite
	- aws or gce
	build, stage, extract, bring up cluster, teardown, etc..
- `kubectl get nodes`
- `kubectl describe node <nodename>`
### Deployments, Rolling Updates, Rollbacks
- Create a deployment .yaml file  
- `kubectl create -f apache-deployment.yaml`
- Update the deployment via command line
- `kubectl set image deployment apache-deployment apache=apache:1.3` apache is the image in the deployment  
- Update by editing the yaml file and applying change
- `kubectl apply apache-deployment.yaml`  
- Rollback a deployment
	- get the rollout history
	- `kubectl rollout history deployment apache-deployment`
	- `kubeclt rollout undo deployment apache-deployment --to-revision=<revNumber>`  

